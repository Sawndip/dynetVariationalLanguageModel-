{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Variational auto encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7efe140a9970>"
      ]
     },
     "execution_count": 512,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parameters:\n",
    "    def __init__(self,  \n",
    "                 word_vocab_size):\n",
    "        \n",
    "        self.word_vocab_size = int(word_vocab_size)\n",
    "        \n",
    "        self.word_embed_size = 300\n",
    "        \n",
    "        self.encoder_rnn_hidden_size = 150\n",
    "        self.encoder_rnn_num_layers = 2\n",
    "        self.encoder_rnn_num_directions = 2\n",
    "        \n",
    "        self.latent_variable_size = 30\n",
    "        \n",
    "        self.decoder_k = 3\n",
    "        self.decoder_dilations = [1, 2, 4]\n",
    "        self.decoder_kernels = [(400, \n",
    "                                 self.latent_variable_size + self.word_embed_size, self.decoder_k), \n",
    "                                (450, 400, self.decoder_k), \n",
    "                                (500, 450, self.decoder_k)]\n",
    "        self.decoder_num_layers = len(self.decoder_kernels)\n",
    "        self.decoder_paddings = [Parameters.effective_k(k, self.decoder_dilations[i])-1\n",
    "                                 for i, (_,_, k) in enumerate(self.decoder_kernels)]\n",
    "    \n",
    "    @staticmethod\n",
    "    def effective_k(k, d):\n",
    "        \"\"\"\n",
    "        :param k: kernel width\n",
    "        :param d: dilation size\n",
    "        :return: effective kernel width after dilation\n",
    "        \"\"\"\n",
    "        return (k-1)*d + 1 # think like (k-1)(d-1) + k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training batch specific data\n",
    "temp_batch_size = 12\n",
    "temp_seq_size = 10\n",
    "temp_words_vocab_size = 10002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = Parameters(temp_words_vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_emb = nn.Embedding(param.word_vocab_size, param.word_embed_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 word_emb_size, \n",
    "                 rnn_hidden_size, \n",
    "                 rnn_num_layers, \n",
    "                 rnn_num_directions=1):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.word_emb_size = word_emb_size\n",
    "        self.rnn_hidden_size = rnn_hidden_size\n",
    "        self.rnn_num_layers = rnn_num_layers\n",
    "        self.rnn_num_directions = rnn_num_directions\n",
    "        self.is_bidirectional = True if self.rnn_num_directions==2 else False\n",
    "        \n",
    "        assert(self.rnn_num_directions==1 or self.rnn_num_directions==2)   \n",
    "        \n",
    "        self.rnn = nn.GRU(input_size=self.word_emb_size, \n",
    "                          hidden_size=self.rnn_hidden_size, \n",
    "                          num_layers=self.rnn_num_layers, \n",
    "                          batch_first=True,\n",
    "                          bidirectional=self.is_bidirectional)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: [batch_size, seq_len, embed_size] tensor\n",
    "        :example x = [[emb_of(<bos>), emb_of(Hi), \n",
    "                       emb_of(there), emb_of(<eos>)]]\n",
    "        :return: last hidden state [batch_size, directions * rnn_hidden_size] tensor\n",
    "        \"\"\"\n",
    "        \n",
    "        batch_size = x.size(0)\n",
    "        # dont think we need to initialize as by default it initializes to zero\n",
    "        #hidden = self._init_hidden(batch_size) \n",
    "        \n",
    "        _, final_state = self.rnn(x) # hidden.shape = (layers*directions, batch_size, rnn_hidden_size)\n",
    "        final_state = final_state.view(self.rnn_num_layers, \n",
    "                                       self.rnn_num_directions, \n",
    "                                       batch_size, \n",
    "                                       self.rnn_hidden_size) \n",
    "        # get the last layer \n",
    "        final_state = final_state[-1] # [rnn_num_directions, batch_size, rnn_hidden_size]\n",
    "    \n",
    "        if self.is_bidirectional:\n",
    "            # if bidirectional, concatenate the directions column wise\n",
    "            final_state = torch.cat((final_state[0], final_state[1]), 1) \n",
    "        else:\n",
    "            # if one directional, get the 0th element, ie the only direction available\n",
    "            final_state = final_state[0]\n",
    "        \n",
    "        return final_state # [batch_size, rnn_num_directions * rnn_hidden_size] tensor\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder dummy input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12, 10, 300])\n"
     ]
    }
   ],
   "source": [
    "encoder_word_input = np.random.randint(low=0, \n",
    "                                       high=param.word_vocab_size, \n",
    "                                       size=(temp_batch_size, temp_seq_size))\n",
    "\n",
    "encoder_word_input = torch.LongTensor(encoder_word_input)\n",
    "encoder_word_input = autograd.Variable(encoder_word_input)\n",
    "encoder_input = word_emb(encoder_word_input)\n",
    "print(encoder_input.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the encoder class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12, 300])\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(param.word_embed_size, \n",
    "                  param.encoder_rnn_hidden_size, \n",
    "                  param.encoder_rnn_num_layers, \n",
    "                  param.encoder_rnn_num_directions)\n",
    "encoder_output = encoder.forward(encoder_input)\n",
    "print(encoder_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, decoder_kernels, dilations, paddings, word_vocab_size):\n",
    "        \"\"\"\n",
    "        :param decoder_kernels: [(out_chan, in_chan, width), ...] has num_layers elements\n",
    "        :param dilations: [1, 2, 4] list of int\n",
    "        :param paddings: [2, 4, 8] list of int\n",
    "        \"\"\"\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.kernels_shape = decoder_kernels\n",
    "        self.dilations = dilations\n",
    "        self.paddings = paddings\n",
    "        self.word_vocab_size = word_vocab_size\n",
    "        \n",
    "        # Because we want to have a variable number of layers, \n",
    "        # I do not know how to use nn.Sequential. \n",
    "        # Because of not being able to use nn.Sequential, we cannot use nn.Conv1d \n",
    "        # and have to use F.conv1d.\n",
    "        \n",
    "        # If we could have used nn.Conv1d, the conv layers would have been in the __init__\n",
    "        # as it would hold the weights. So we would not have to store the \n",
    "        # weights explicitly in self.kernels_param/self.biases_param\n",
    "        \n",
    "        # Learnable kernel parameters\n",
    "        self.kernels_param = [nn.Parameter(torch.Tensor(out_chan, in_chan, width).normal_(0, 0.05))\n",
    "                              for out_chan, in_chan, width in decoder_kernels]\n",
    "        self._add_to_parameters(self.kernels_param, \"decoder_kernels_param\")\n",
    "        \n",
    "        # Learnable bias parameters\n",
    "        self.biases_param = [nn.Parameter(torch.Tensor(out_chan).normal_(0, 0.05)) \n",
    "                             for out_chan, _, _ in decoder_kernels ]\n",
    "        self._add_to_parameters(self.biases_param, \"decoder_biases_param\")\n",
    "        \n",
    "        self.conv_out_size = self.kernels_shape[-1][0]\n",
    "        self.lin_layer = nn.Linear(self.conv_out_size, words_vocab_size)\n",
    "        \n",
    "        \n",
    "    def _add_to_parameters(self, parameters, name):\n",
    "        # Necessary to do this for the module to access the parameters\n",
    "        for i, parameter in enumerate(parameters):\n",
    "            self.register_parameter(name='{}-{}'.format(name, i), param=parameter)\n",
    "        \n",
    "    def forward(self, x, z=None):\n",
    "        \"\"\"\n",
    "        :param x: [batch_size, seq_len, word_emb_size]\n",
    "        :param z: [batch_size, lat_var_size]\n",
    "        \n",
    "        :note: for x, the last element of the seq is <eos>\n",
    "        :return: un-normalized logit of sentence words \n",
    "                 distribution\n",
    "                 [batch_size, seq_len, word_vocab_size]\n",
    "        \"\"\"\n",
    "        \n",
    "        decoder_input = x[:,:-1,:] # last in seq is <eos> which is not fed as input\n",
    "        \n",
    "        lat_var_size = z.shape[1] # [batch_size, lat_var_size]\n",
    "        batch_size, input_seq_len, word_emb_size = decoder_input.shape\n",
    "    \n",
    "        z = torch.cat([z]*input_seq_len, 1) # [batch_size, lat_var_size * input_seq_len]\n",
    "        z = z.view(batch_size, input_seq_len, lat_var_size) # [batch_size, input_seq_len, lat_var_size]\n",
    "        \n",
    "        # concatenate z to each word in the input for decoder\n",
    "        decoder_input = torch.cat([decoder_input, z], 2) # [batch_size, input_seq_len, word_emb_size + lat_var_size] \n",
    "            \n",
    "        \"\"\"\n",
    "        Why transpose:\n",
    "        Input decoder_input has the shape of [batch_size, input_seq_len, word_emb_size + lat_var_size]\n",
    "        Since Conv1d takes in input in the form [batch_size, word_emb_size + lat_var_size, input_seq_len], \n",
    "        we need to change shape.\n",
    "        \n",
    "        Why contiguous:\n",
    "        Below, we use \"contiguous\" to store the variable in contiguous memory. \n",
    "        Storing variable in contiguous memory is necessary to call \"view\" on the variable.\n",
    "        \"\"\"\n",
    "        # [batch_size, word_emb_size + lat_var_size, input_seq_len]\n",
    "        decoder_input = decoder_input.transpose(1, 2).contiguous() \n",
    "        \n",
    "        x = decoder_input\n",
    "        # Get the output from the conv layer\n",
    "        for layer, kernel in enumerate(self.kernels_shape):\n",
    "            out_chan, in_chan, width = kernel[0], kernel[1], kernel[2] \n",
    "            \n",
    "            pad = self.paddings[layer]\n",
    "            dil = self.dilations[layer]\n",
    "            \n",
    "            x = F.conv1d(x, self.kernels_param[layer], \n",
    "                               bias=self.biases_param[layer], \n",
    "                               dilation=self.dilations[layer], \n",
    "                               padding=self.paddings[layer])\n",
    "            \n",
    "            # Because of padding, the seq_len increases by |padding|\n",
    "            # These are not meaningful in language modeling and needs to be removed\n",
    "            x = x[:,:,:-self.paddings[layer]].contiguous()\n",
    "            x = F.relu(x)\n",
    "            \n",
    "        # x.shape = [batch_size, self.conv_out_size, input_seq_len]\n",
    "        print(x.shape)\n",
    "            \n",
    "        # Return should have size [batch_size, input_seq_len, self.conv_out_size]\n",
    "        x = x.transpose(1, 2).contiguous() # [batch_size, input_seq_len, self.conv_out_size]\n",
    "        \n",
    "        # Only the out_chal should go through the linear layer\n",
    "        x = x.view(-1, self.conv_out_size) # [batch_size * input_seq_len, self.conv_out_size]\n",
    "        x = self.lin_layer(x) # [batch_size * input_seq_len, self.word_vocab_size]\n",
    "        logits = x.view(-1, input_seq_len, self.word_vocab_size) # [batch_size * input_seq_len, self.word_vocab_size]\n",
    "        return logits \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder dummy input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_word_input = np.random.randint(low=0, \n",
    "                                       high=temp_word_vocab_size, \n",
    "                                       size=(temp_batch_size, temp_seq_size))\n",
    "\n",
    "decoder_word_input = torch.LongTensor(decoder_word_input)\n",
    "decoder_word_input = autograd.Variable(decoder_word_input)\n",
    "word_emb = nn.Embedding(temp_word_vocab_size, param.word_embed_size)\n",
    "decoder_input = word_emb(decoder_word_input)\n",
    "\n",
    "z = autograd.Variable(torch.randn([temp_batch_size, param.latent_variable_size]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the decoder class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12, 10, 300])\n",
      "torch.Size([12, 500, 9])\n"
     ]
    }
   ],
   "source": [
    "decoder = Decoder(param.decoder_kernels, \n",
    "                  param.decoder_dilations, \n",
    "                  param.decoder_paddings, \n",
    "                  temp_words_vocab_size)\n",
    "print(decoder_input.shape)\n",
    "logits = decoder.forward(decoder_input, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
